#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åˆ›å»ºYOLOè®­ç»ƒæ•°æ®é›†é…ç½®æ–‡ä»¶
æ­¤è„šæœ¬ç”¨äºä»æ£€æµ‹ç»“æœç”Ÿæˆçš„datasetç›®å½•åˆ›å»ºYOLOè®­ç»ƒé…ç½®æ–‡ä»¶
"""

import os
import yaml
import argparse
from pathlib import Path
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def create_dataset_yaml(dataset_dir, output_path="dataset.yaml", class_names=None, train_split=0.8):
    """
    åˆ›å»ºYOLOè®­ç»ƒç”¨çš„æ•°æ®é›†é…ç½®æ–‡ä»¶
    
    Args:
        dataset_dir (str): train_datasetç›®å½•è·¯å¾„
        output_path (str): è¾“å‡ºYAMLæ–‡ä»¶è·¯å¾„
        class_names (list): ç±»åˆ«åç§°åˆ—è¡¨
        train_split (float): è®­ç»ƒé›†æ¯”ä¾‹
    """
    try:
        dataset_path = Path(dataset_dir)
        images_dir = dataset_path / "images"
        labels_dir = dataset_path / "labels"
        
        if not images_dir.exists() or not labels_dir.exists():
            logger.error(f"train_datasetç›®å½•ç»“æ„ä¸æ­£ç¡®: {dataset_path}")
            logger.error(f"éœ€è¦å­˜åœ¨: {images_dir} å’Œ {labels_dir}")
            return None
        
        # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
        image_files = list(images_dir.glob("*.*"))
        label_files = list(labels_dir.glob("*.txt"))
        
        logger.info(f"æ‰¾åˆ° {len(image_files)} ä¸ªå›¾åƒæ–‡ä»¶")
        logger.info(f"æ‰¾åˆ° {len(label_files)} ä¸ªæ ‡ç­¾æ–‡ä»¶")
        
        # å¦‚æœæœªæä¾›ç±»åˆ«åç§°ï¼Œå°è¯•ä»æ ‡ç­¾æ–‡ä»¶ä¸­æå–
        if class_names is None:
            class_ids = set()
            for label_file in label_files:
                try:
                    with open(label_file, 'r', encoding='utf-8') as f:
                        for line in f:
                            parts = line.strip().split()
                            if parts:
                                class_ids.add(int(parts[0]))
                except Exception as e:
                    logger.warning(f"è¯»å–æ ‡ç­¾æ–‡ä»¶å¤±è´¥ {label_file}: {e}")
            
            # ç”Ÿæˆé»˜è®¤ç±»åˆ«åç§°
            class_names = [f"class_{i}" for i in sorted(class_ids)]
            logger.info(f"ä»æ ‡ç­¾æ–‡ä»¶ä¸­æ£€æµ‹åˆ° {len(class_names)} ä¸ªç±»åˆ«: {class_names}")
            logger.warning("è¯·æ‰‹åŠ¨ç¼–è¾‘YAMLæ–‡ä»¶ï¼Œå°†class_0, class_1ç­‰æ›¿æ¢ä¸ºå®é™…çš„ç±»åˆ«åç§°")
        
        # åˆ›å»ºæ•°æ®é›†é…ç½®
        dataset_config = {
            'path': str(dataset_path.resolve()),
            'train': 'images',
            'val': 'images',  # å¯ä»¥åç»­æ‰‹åŠ¨åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
            'test': 'images',
            'nc': len(class_names),
            'names': class_names
        }
        
        # ä¿å­˜YAMLé…ç½®æ–‡ä»¶
        output_file = Path(output_path)
        with open(output_file, 'w', encoding='utf-8') as f:
            yaml.dump(dataset_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
        
        logger.info("=" * 60)
        logger.info("ğŸ¯ è®­ç»ƒæ•°æ®é›†é…ç½®æ–‡ä»¶åˆ›å»ºæˆåŠŸ!")
        logger.info(f"ğŸ“ é…ç½®æ–‡ä»¶è·¯å¾„: {output_file.resolve()}")
        logger.info(f"ğŸ“Š train_datasetè·¯å¾„: {dataset_path.resolve()}")
        logger.info(f"ğŸ·ï¸  ç±»åˆ«æ•°é‡: {len(class_names)}")
        logger.info(f"ğŸ“· å›¾åƒæ•°é‡: {len(image_files)}")
        logger.info(f"ğŸ“ æ ‡ç­¾æ•°é‡: {len(label_files)}")
        logger.info("=" * 60)
        
        # æ˜¾ç¤ºé…ç½®æ–‡ä»¶å†…å®¹
        logger.info("ğŸ“‹ ç”Ÿæˆçš„é…ç½®æ–‡ä»¶å†…å®¹:")
        with open(output_file, 'r', encoding='utf-8') as f:
            content = f.read()
            for line in content.split('\n'):
                if line.strip():
                    logger.info(f"  {line}")
        
        logger.info("=" * 60)
        logger.info("ğŸ“ ä½¿ç”¨è¯´æ˜:")
        logger.info("1. è¯·æ£€æŸ¥å¹¶ä¿®æ”¹ç±»åˆ«åç§°ï¼ˆå°†class_0ç­‰æ›¿æ¢ä¸ºå®é™…åç§°ï¼‰")
        logger.info("2. å¦‚éœ€åˆ†å‰²è®­ç»ƒ/éªŒè¯é›†ï¼Œè¯·åˆ›å»ºtrainå’Œvalå­ç›®å½•")
        logger.info(f"3. ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¼€å§‹è®­ç»ƒ:")
        logger.info(f"   python train_yolo.py --data {output_file} --epochs 100")
        
        return str(output_file)
        
    except Exception as e:
        logger.error(f"åˆ›å»ºé…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
        return None

def split_dataset(dataset_dir, train_ratio=0.8, val_ratio=0.15):
    """
    å°†train_datasetæ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
    
    Args:
        dataset_dir (str): train_datasetç›®å½•è·¯å¾„
        train_ratio (float): è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio (float): éªŒè¯é›†æ¯”ä¾‹
    """
    try:
        import random
        import shutil
        
        dataset_path = Path(dataset_dir)
        images_dir = dataset_path / "images"
        labels_dir = dataset_path / "labels"
        
        # åˆ›å»ºåˆ†å‰²åçš„ç›®å½•ç»“æ„
        train_images_dir = dataset_path / "train" / "images"
        train_labels_dir = dataset_path / "train" / "labels"
        val_images_dir = dataset_path / "val" / "images"
        val_labels_dir = dataset_path / "val" / "labels"
        test_images_dir = dataset_path / "test" / "images"
        test_labels_dir = dataset_path / "test" / "labels"
        
        for dir_path in [train_images_dir, train_labels_dir, val_images_dir, 
                        val_labels_dir, test_images_dir, test_labels_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = list(images_dir.glob("*.*"))
        random.shuffle(image_files)
        
        total_count = len(image_files)
        train_count = int(total_count * train_ratio)
        val_count = int(total_count * val_ratio)
        
        logger.info(f"æ•°æ®é›†åˆ†å‰²:")
        logger.info(f"  è®­ç»ƒé›†: {train_count} å¼  ({train_ratio*100:.1f}%)")
        logger.info(f"  éªŒè¯é›†: {val_count} å¼  ({val_ratio*100:.1f}%)")
        logger.info(f"  æµ‹è¯•é›†: {total_count - train_count - val_count} å¼ ")
        
        # åˆ†å‰²æ•°æ®é›†
        for i, image_file in enumerate(image_files):
            label_file = labels_dir / f"{image_file.stem}.txt"
            
            if i < train_count:
                # è®­ç»ƒé›†
                shutil.copy(image_file, train_images_dir)
                if label_file.exists():
                    shutil.copy(label_file, train_labels_dir)
            elif i < train_count + val_count:
                # éªŒè¯é›†
                shutil.copy(image_file, val_images_dir)
                if label_file.exists():
                    shutil.copy(label_file, val_labels_dir)
            else:
                # æµ‹è¯•é›†
                shutil.copy(image_file, test_images_dir)
                if label_file.exists():
                    shutil.copy(label_file, test_labels_dir)
        
        logger.info("âœ… æ•°æ®é›†åˆ†å‰²å®Œæˆ!")
        return True
        
    except Exception as e:
        logger.error(f"æ•°æ®é›†åˆ†å‰²å¤±è´¥: {str(e)}")
        return False

def main():
    """å‘½ä»¤è¡Œå…¥å£å‡½æ•°"""
    parser = argparse.ArgumentParser(description="åˆ›å»ºYOLOè®­ç»ƒæ•°æ®é›†é…ç½®æ–‡ä»¶")
    parser.add_argument("--train_dataset_dir", type=str, required=True, help="train_datasetç›®å½•è·¯å¾„")
    parser.add_argument("--output", type=str, default="dataset.yaml", help="è¾“å‡ºYAMLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--class_names", nargs='+', help="ç±»åˆ«åç§°åˆ—è¡¨")
    parser.add_argument("--split", action="store_true", help="æ˜¯å¦åˆ†å‰²æ•°æ®é›†")
    parser.add_argument("--train_ratio", type=float, default=0.8, help="è®­ç»ƒé›†æ¯”ä¾‹")
    parser.add_argument("--val_ratio", type=float, default=0.15, help="éªŒè¯é›†æ¯”ä¾‹")
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    config_path = create_dataset_yaml(
        args.train_dataset_dir, 
        args.output, 
        args.class_names
    )
    
    if config_path and args.split:
        # åˆ†å‰²æ•°æ®é›†
        split_dataset(args.train_dataset_dir, args.train_ratio, args.val_ratio)
        
        # æ›´æ–°é…ç½®æ–‡ä»¶è·¯å¾„
        dataset_path = Path(args.train_dataset_dir)
        updated_config = {
            'path': str(dataset_path.resolve()),
            'train': 'train/images',
            'val': 'val/images',
            'test': 'test/images',
            'nc': len(args.class_names) if args.class_names else 1,
            'names': args.class_names if args.class_names else ['class_0']
        }
        
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(updated_config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
        
        logger.info(f"âœ… å·²æ›´æ–°é…ç½®æ–‡ä»¶ä»¥æ”¯æŒåˆ†å‰²åçš„æ•°æ®é›†ç»“æ„")

if __name__ == "__main__":
    main()
